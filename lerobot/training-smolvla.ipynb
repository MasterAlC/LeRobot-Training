{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUk3Y0WwYZ4"
      },
      "source": [
        "# ðŸ¤— x ðŸ¦¾: Training SmolVLA with LeRobot Notebook\n",
        "\n",
        "Welcome to the **LeRobot SmolVLA training notebook**! This notebook provides a ready-to-run setup for training imitation learning policies using the [ðŸ¤— LeRobot](https://github.com/huggingface/lerobot) library.\n",
        "\n",
        "In this example, we train an `SmolVLA` policy using a dataset hosted on the [Hugging Face Hub](https://huggingface.co/), and optionally track training metrics with [Weights & Biases (wandb)](https://wandb.ai/).\n",
        "\n",
        "## âš™ï¸ Requirements\n",
        "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=YOUR_USERNAME/YOUR_DATASET`)\n",
        "- Optional: A [wandb](https://wandb.ai/) account if you want to enable training visualization\n",
        "- Recommended: GPU runtime (e.g., NVIDIA A100) for faster training\n",
        "\n",
        "## â±ï¸ Expected Training Time\n",
        "Training with the `SmolVLA` policy for 20,000 steps typically takes **about 5 hours on an NVIDIA A100** GPU. On less powerful GPUs or CPUs, training may take significantly longer!\n",
        "\n",
        "## Example Output\n",
        "Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`. If `wandb` is enabled, progress will also be visualized in your wandb project dashboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOJyX0CnwA5m"
      },
      "source": [
        "## Install conda\n",
        "This cell uses `condacolab` to bootstrap a full Conda environment inside Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlKjL1X5t_zM",
        "outputId": "e1e07d58-acc9-4999-c141-2e0047b56f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxCc3CARwUjN"
      },
      "source": [
        "## Install LeRobot\n",
        "This cell clones the `lerobot` repository from Hugging Face, installs FFmpeg (version 7.1.1), and installs the package in editable mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgLu7QT5tUik",
        "outputId": "953d9d3e-7324-49fd-8889-f19020f84a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lerobot'...\n",
            "remote: Enumerating objects: 42518, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 42518 (delta 248), reused 199 (delta 199), pack-reused 42238 (from 3)\u001b[K\n",
            "Receiving objects: 100% (42518/42518), 206.23 MiB | 15.80 MiB/s, done.\n",
            "Resolving deltas: 100% (27423/27423), done.\n",
            "Filtering content: 100% (45/45), 69.03 MiB | 1.17 MiB/s, done.\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ffmpeg=7.1.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    alsa-lib-1.2.14            |       hb9d3cd8_0         553 KB  conda-forge\n",
            "    aom-3.9.1                  |       hac33072_0         2.6 MB  conda-forge\n",
            "    attr-2.5.2                 |       h39aace5_0          66 KB  conda-forge\n",
            "    ca-certificates-2025.11.12 |       hbd8a1cb_0         149 KB  conda-forge\n",
            "    cairo-1.18.4               |       h3394656_0         955 KB  conda-forge\n",
            "    certifi-2025.11.12         |     pyhd8ed1ab_0         153 KB  conda-forge\n",
            "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
            "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
            "    dbus-1.16.2                |       h3c4dab8_0         428 KB  conda-forge\n",
            "    ffmpeg-7.1.1               | gpl_h5c0ada0_711        10.1 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
            "    fontconfig-2.15.0          |       h7e30c49_1         259 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |       hc364b38_1           4 KB  conda-forge\n",
            "    freetype-2.14.1            |       ha770c72_0         169 KB  conda-forge\n",
            "    fribidi-1.0.16             |       hb03c661_0          60 KB  conda-forge\n",
            "    gdk-pixbuf-2.44.4          |       h2b0a6b4_0         559 KB  conda-forge\n",
            "    gettext-0.25.1             |       h3f43e3d_1         529 KB  conda-forge\n",
            "    gettext-tools-0.25.1       |       h3f43e3d_1         3.5 MB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    graphite2-1.3.14           |       hecca717_2          97 KB  conda-forge\n",
            "    harfbuzz-12.2.0            |       h15599e2_0         2.3 MB  conda-forge\n",
            "    icu-75.1                   |       he02047a_0        11.6 MB  conda-forge\n",
            "    intel-gmmlib-22.8.2        |       hb700be7_0         976 KB  conda-forge\n",
            "    intel-media-driver-25.3.4  |       hecca717_0         8.0 MB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h0aef613_1         258 KB  conda-forge\n",
            "    level-zero-1.26.0          |       hb700be7_0         624 KB  conda-forge\n",
            "    libabseil-20250512.1       | cxx17_hba17884_0         1.2 MB  conda-forge\n",
            "    libasprintf-0.25.1         |       h3f43e3d_1          52 KB  conda-forge\n",
            "    libasprintf-devel-0.25.1   |       h3f43e3d_1          34 KB  conda-forge\n",
            "    libass-0.17.4              |       h96ad9f0_0         149 KB  conda-forge\n",
            "    libcap-2.77                |       h3ff7636_0         119 KB  conda-forge\n",
            "    libdeflate-1.25            |       h17f619e_0          72 KB  conda-forge\n",
            "    libdrm-2.4.125             |       hb03c661_1         304 KB  conda-forge\n",
            "    libegl-1.7.0               |       ha4b6fd6_2          44 KB  conda-forge\n",
            "    libexpat-2.7.3             |       hecca717_0          75 KB  conda-forge\n",
            "    libffi-3.5.2               |       h9ec8514_0          56 KB  conda-forge\n",
            "    libflac-1.4.3              |       h59595ed_0         385 KB  conda-forge\n",
            "    libfreetype-2.14.1         |       ha770c72_0           7 KB  conda-forge\n",
            "    libfreetype6-2.14.1        |       h73754d4_0         378 KB  conda-forge\n",
            "    libgettextpo-0.25.1        |       h3f43e3d_1         184 KB  conda-forge\n",
            "    libgettextpo-devel-0.25.1  |       h3f43e3d_1          37 KB  conda-forge\n",
            "    libgl-1.7.0                |       ha4b6fd6_2         132 KB  conda-forge\n",
            "    libglib-2.86.1             |       h32235b2_2         3.8 MB  conda-forge\n",
            "    libglvnd-1.7.0             |       ha4b6fd6_2         129 KB  conda-forge\n",
            "    libglx-1.7.0               |       ha4b6fd6_2          74 KB  conda-forge\n",
            "    libhwloc-2.12.1            |default_h7f8ec31_1002         2.3 MB  conda-forge\n",
            "    libiconv-1.18              |       h3b78370_2         772 KB  conda-forge\n",
            "    libjpeg-turbo-3.1.2        |       hb03c661_0         619 KB  conda-forge\n",
            "    liblzma-5.8.1              |       hb9d3cd8_2         110 KB  conda-forge\n",
            "    libogg-1.3.5               |       hd0c01bc_1         213 KB  conda-forge\n",
            "    libopenvino-2025.2.0       |       hb617929_1         6.0 MB  conda-forge\n",
            "    libopenvino-auto-batch-plugin-2025.2.0|       hed573e4_1         112 KB  conda-forge\n",
            "    libopenvino-auto-plugin-2025.2.0|       hed573e4_1         245 KB  conda-forge\n",
            "    libopenvino-hetero-plugin-2025.2.0|       hd41364c_1         190 KB  conda-forge\n",
            "    libopenvino-intel-cpu-plugin-2025.2.0|       hb617929_1        11.8 MB  conda-forge\n",
            "    libopenvino-intel-gpu-plugin-2025.2.0|       hb617929_1        10.3 MB  conda-forge\n",
            "    libopenvino-intel-npu-plugin-2025.2.0|       hb617929_1         1.2 MB  conda-forge\n",
            "    libopenvino-ir-frontend-2025.2.0|       hd41364c_1         200 KB  conda-forge\n",
            "    libopenvino-onnx-frontend-2025.2.0|       h1862bb8_1         1.6 MB  conda-forge\n",
            "    libopenvino-paddle-frontend-2025.2.0|       h1862bb8_1         727 KB  conda-forge\n",
            "    libopenvino-pytorch-frontend-2025.2.0|       hecca717_1         1.2 MB  conda-forge\n",
            "    libopenvino-tensorflow-frontend-2025.2.0|       h0767aad_1         1.3 MB  conda-forge\n",
            "    libopenvino-tensorflow-lite-frontend-2025.2.0|       hecca717_1         485 KB  conda-forge\n",
            "    libopus-1.5.2              |       hd0c01bc_0         305 KB  conda-forge\n",
            "    libpciaccess-0.18          |       hb9d3cd8_0          28 KB  conda-forge\n",
            "    libpng-1.6.50              |       h421ea60_1         310 KB  conda-forge\n",
            "    libprotobuf-6.31.1         |       h49aed37_2         4.4 MB  conda-forge\n",
            "    librsvg-2.60.0             |       h61e6d4b_0         3.3 MB  conda-forge\n",
            "    libsndfile-1.2.2           |       hc60ed4a_1         346 KB  conda-forge\n",
            "    libsystemd0-258.2          |       h6569c3e_1         510 KB  conda-forge\n",
            "    libtiff-4.7.1              |       h9d88235_1         425 KB  conda-forge\n",
            "    libudev1-258.2             |       h6569c3e_1         163 KB  conda-forge\n",
            "    libunwind-1.8.3            |       h65a8314_0          74 KB  conda-forge\n",
            "    liburing-2.12              |       hb700be7_0         125 KB  conda-forge\n",
            "    libusb-1.0.29              |       h73b1eb8_0          87 KB  conda-forge\n",
            "    libva-2.22.0               |       h4f16b4b_2         212 KB  conda-forge\n",
            "    libvorbis-1.3.7            |       h54a6638_2         279 KB  conda-forge\n",
            "    libvpl-2.15.0              |       h54a6638_1         281 KB  conda-forge\n",
            "    libvpx-1.14.1              |       hac33072_0         999 KB  conda-forge\n",
            "    libvulkan-loader-1.4.328.1 |       h5279c79_0         193 KB  conda-forge\n",
            "    libwebp-base-1.6.0         |       hd42ef1d_0         419 KB  conda-forge\n",
            "    libxcb-1.17.0              |       h8a09558_0         387 KB  conda-forge\n",
            "    libxkbcommon-1.13.0        |       hca5e8e5_0         824 KB  conda-forge\n",
            "    libxml2-2.15.1             |       h26afc86_0          44 KB  conda-forge\n",
            "    libxml2-16-2.15.1          |       ha9997c6_0         543 KB  conda-forge\n",
            "    mpg123-1.32.9              |       hc50e24c_0         480 KB  conda-forge\n",
            "    ocl-icd-2.3.3              |       hb9d3cd8_0         104 KB  conda-forge\n",
            "    opencl-headers-2025.06.13  |       h5888daf_0          54 KB  conda-forge\n",
            "    openh264-2.6.0             |       hc22cd8d_0         714 KB  conda-forge\n",
            "    openssl-3.6.0              |       h26f9b46_0         3.0 MB  conda-forge\n",
            "    pango-1.56.4               |       hadf4263_0         445 KB  conda-forge\n",
            "    pcre2-10.46                |       h1321c63_0         1.2 MB  conda-forge\n",
            "    pixman-0.46.4              |       h54a6638_1         440 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
            "    pugixml-1.15               |       h3f63f65_0         116 KB  conda-forge\n",
            "    pulseaudio-client-17.0     |       h9a6aba3_3         733 KB  conda-forge\n",
            "    sdl2-2.32.56               |       h54a6638_0         575 KB  conda-forge\n",
            "    sdl3-3.2.26                |       h68140b3_0         1.8 MB  conda-forge\n",
            "    snappy-1.2.2               |       h03e3b7b_1          45 KB  conda-forge\n",
            "    svt-av1-3.1.2              |       hecca717_0         2.6 MB  conda-forge\n",
            "    tbb-2022.3.0               |       h8d10470_1         177 KB  conda-forge\n",
            "    wayland-1.24.0             |       hd6090a7_1         322 KB  conda-forge\n",
            "    wayland-protocols-1.45     |       hd8ed1ab_0         135 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xkeyboard-config-2.46      |       hb03c661_0         388 KB  conda-forge\n",
            "    xorg-libice-1.1.2          |       hb9d3cd8_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.6           |       he73a12e_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.12         |       h4f16b4b_0         816 KB  conda-forge\n",
            "    xorg-libxau-1.0.12         |       hb03c661_1          15 KB  conda-forge\n",
            "    xorg-libxcursor-1.2.3      |       hb9d3cd8_0          32 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.5        |       hb03c661_1          20 KB  conda-forge\n",
            "    xorg-libxext-1.3.6         |       hb9d3cd8_0          49 KB  conda-forge\n",
            "    xorg-libxfixes-6.0.2       |       hb03c661_0          20 KB  conda-forge\n",
            "    xorg-libxrandr-1.5.4       |       hb9d3cd8_0          29 KB  conda-forge\n",
            "    xorg-libxrender-0.9.12     |       hb9d3cd8_0          32 KB  conda-forge\n",
            "    xorg-libxscrnsaver-1.2.4   |       hb9d3cd8_0          14 KB  conda-forge\n",
            "    zstandard-0.25.0           |  py311haee01d2_1         456 KB  conda-forge\n",
            "    zstd-1.5.7                 |       hb8e6e7a_2         554 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       129.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.14-hb9d3cd8_0 \n",
            "  aom                conda-forge/linux-64::aom-3.9.1-hac33072_0 \n",
            "  attr               conda-forge/linux-64::attr-2.5.2-h39aace5_0 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.4-h3394656_0 \n",
            "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
            "  dbus               conda-forge/linux-64::dbus-1.16.2-h3c4dab8_0 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-7.1.1-gpl_h5c0ada0_711 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.15.0-h7e30c49_1 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-hc364b38_1 \n",
            "  freetype           conda-forge/linux-64::freetype-2.14.1-ha770c72_0 \n",
            "  fribidi            conda-forge/linux-64::fribidi-1.0.16-hb03c661_0 \n",
            "  gdk-pixbuf         conda-forge/linux-64::gdk-pixbuf-2.44.4-h2b0a6b4_0 \n",
            "  gettext            conda-forge/linux-64::gettext-0.25.1-h3f43e3d_1 \n",
            "  gettext-tools      conda-forge/linux-64::gettext-tools-0.25.1-h3f43e3d_1 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  graphite2          conda-forge/linux-64::graphite2-1.3.14-hecca717_2 \n",
            "  harfbuzz           conda-forge/linux-64::harfbuzz-12.2.0-h15599e2_0 \n",
            "  icu                conda-forge/linux-64::icu-75.1-he02047a_0 \n",
            "  intel-gmmlib       conda-forge/linux-64::intel-gmmlib-22.8.2-hb700be7_0 \n",
            "  intel-media-driver conda-forge/linux-64::intel-media-driver-25.3.4-hecca717_0 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h0aef613_1 \n",
            "  level-zero         conda-forge/linux-64::level-zero-1.26.0-hb700be7_0 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20250512.1-cxx17_hba17884_0 \n",
            "  libasprintf        conda-forge/linux-64::libasprintf-0.25.1-h3f43e3d_1 \n",
            "  libasprintf-devel  conda-forge/linux-64::libasprintf-devel-0.25.1-h3f43e3d_1 \n",
            "  libass             conda-forge/linux-64::libass-0.17.4-h96ad9f0_0 \n",
            "  libcap             conda-forge/linux-64::libcap-2.77-h3ff7636_0 \n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.25-h17f619e_0 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.125-hb03c661_1 \n",
            "  libegl             conda-forge/linux-64::libegl-1.7.0-ha4b6fd6_2 \n",
            "  libflac            conda-forge/linux-64::libflac-1.4.3-h59595ed_0 \n",
            "  libfreetype        conda-forge/linux-64::libfreetype-2.14.1-ha770c72_0 \n",
            "  libfreetype6       conda-forge/linux-64::libfreetype6-2.14.1-h73754d4_0 \n",
            "  libgettextpo       conda-forge/linux-64::libgettextpo-0.25.1-h3f43e3d_1 \n",
            "  libgettextpo-devel conda-forge/linux-64::libgettextpo-devel-0.25.1-h3f43e3d_1 \n",
            "  libgl              conda-forge/linux-64::libgl-1.7.0-ha4b6fd6_2 \n",
            "  libglib            conda-forge/linux-64::libglib-2.86.1-h32235b2_2 \n",
            "  libglvnd           conda-forge/linux-64::libglvnd-1.7.0-ha4b6fd6_2 \n",
            "  libglx             conda-forge/linux-64::libglx-1.7.0-ha4b6fd6_2 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.12.1-default_h7f8ec31_1002 \n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.1.2-hb03c661_0 \n",
            "  libogg             conda-forge/linux-64::libogg-1.3.5-hd0c01bc_1 \n",
            "  libopenvino        conda-forge/linux-64::libopenvino-2025.2.0-hb617929_1 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2025.2.0-hed573e4_1 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2025.2.0-hed573e4_1 \n",
            "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2025.2.0-hd41364c_1 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2025.2.0-hb617929_1 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2025.2.0-hb617929_1 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-npu-plugin-2025.2.0-hb617929_1 \n",
            "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2025.2.0-hd41364c_1 \n",
            "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2025.2.0-h1862bb8_1 \n",
            "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2025.2.0-h1862bb8_1 \n",
            "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2025.2.0-hecca717_1 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2025.2.0-h0767aad_1 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2025.2.0-hecca717_1 \n",
            "  libopus            conda-forge/linux-64::libopus-1.5.2-hd0c01bc_0 \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.18-hb9d3cd8_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.50-h421ea60_1 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-6.31.1-h49aed37_2 \n",
            "  librsvg            conda-forge/linux-64::librsvg-2.60.0-h61e6d4b_0 \n",
            "  libsndfile         conda-forge/linux-64::libsndfile-1.2.2-hc60ed4a_1 \n",
            "  libsystemd0        conda-forge/linux-64::libsystemd0-258.2-h6569c3e_1 \n",
            "  libtiff            conda-forge/linux-64::libtiff-4.7.1-h9d88235_1 \n",
            "  libudev1           conda-forge/linux-64::libudev1-258.2-h6569c3e_1 \n",
            "  libunwind          conda-forge/linux-64::libunwind-1.8.3-h65a8314_0 \n",
            "  liburing           conda-forge/linux-64::liburing-2.12-hb700be7_0 \n",
            "  libusb             conda-forge/linux-64::libusb-1.0.29-h73b1eb8_0 \n",
            "  libva              conda-forge/linux-64::libva-2.22.0-h4f16b4b_2 \n",
            "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h54a6638_2 \n",
            "  libvpl             conda-forge/linux-64::libvpl-2.15.0-h54a6638_1 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.14.1-hac33072_0 \n",
            "  libvulkan-loader   conda-forge/linux-64::libvulkan-loader-1.4.328.1-h5279c79_0 \n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.6.0-hd42ef1d_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.17.0-h8a09558_0 \n",
            "  libxkbcommon       conda-forge/linux-64::libxkbcommon-1.13.0-hca5e8e5_0 \n",
            "  libxml2-16         conda-forge/linux-64::libxml2-16-2.15.1-ha9997c6_0 \n",
            "  mpg123             conda-forge/linux-64::mpg123-1.32.9-hc50e24c_0 \n",
            "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.3-hb9d3cd8_0 \n",
            "  opencl-headers     conda-forge/linux-64::opencl-headers-2025.06.13-h5888daf_0 \n",
            "  openh264           conda-forge/linux-64::openh264-2.6.0-hc22cd8d_0 \n",
            "  pango              conda-forge/linux-64::pango-1.56.4-hadf4263_0 \n",
            "  pcre2              conda-forge/linux-64::pcre2-10.46-h1321c63_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.46.4-h54a6638_1 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
            "  pugixml            conda-forge/linux-64::pugixml-1.15-h3f63f65_0 \n",
            "  pulseaudio-client  conda-forge/linux-64::pulseaudio-client-17.0-h9a6aba3_3 \n",
            "  sdl2               conda-forge/linux-64::sdl2-2.32.56-h54a6638_0 \n",
            "  sdl3               conda-forge/linux-64::sdl3-3.2.26-h68140b3_0 \n",
            "  snappy             conda-forge/linux-64::snappy-1.2.2-h03e3b7b_1 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-3.1.2-hecca717_0 \n",
            "  tbb                conda-forge/linux-64::tbb-2022.3.0-h8d10470_1 \n",
            "  wayland            conda-forge/linux-64::wayland-1.24.0-hd6090a7_1 \n",
            "  wayland-protocols  conda-forge/noarch::wayland-protocols-1.45-hd8ed1ab_0 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xkeyboard-config   conda-forge/linux-64::xkeyboard-config-2.46-hb03c661_0 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.2-hb9d3cd8_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.6-he73a12e_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.12-h4f16b4b_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb03c661_1 \n",
            "  xorg-libxcursor    conda-forge/linux-64::xorg-libxcursor-1.2.3-hb9d3cd8_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb03c661_1 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.6-hb9d3cd8_0 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-6.0.2-hb03c661_0 \n",
            "  xorg-libxrandr     conda-forge/linux-64::xorg-libxrandr-1.5.4-hb9d3cd8_0 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.12-hb9d3cd8_0 \n",
            "  xorg-libxscrnsaver conda-forge/linux-64::xorg-libxscrnsaver-1.2.4-hb9d3cd8_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
            "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2025.11.12-pyhd8ed1ab_0 \n",
            "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
            "  libexpat                                 2.6.4-h5888daf_0 --> 2.7.3-hecca717_0 \n",
            "  libffi                                   3.4.2-h7f98852_5 --> 3.5.2-h9ec8514_0 \n",
            "  libiconv                                  1.17-hd590300_2 --> 1.18-h3b78370_2 \n",
            "  liblzma                                  5.6.3-hb9d3cd8_1 --> 5.8.1-hb9d3cd8_2 \n",
            "  libxml2                                 2.13.5-h0d44e9d_1 --> 2.15.1-h26afc86_0 \n",
            "  openssl                                  3.4.0-h7b32b05_1 --> 3.6.0-h26f9b46_0 \n",
            "  zstandard                          0.23.0-py311hbc35293_1 --> 0.25.0-py311haee01d2_1 \n",
            "  zstd                                     1.5.6-ha6fb4c9_0 --> 1.5.7-hb8e6e7a_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "libopenvino-intel-cp | 11.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "icu-75.1             | 11.6 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "intel-media-driver-2 | 8.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-2025.2.0 | 6.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-6.31.1   | 4.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.86.1       | 3.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gettext-tools-0.25.1 | 3.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "librsvg-2.60.0       | 3.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-3.1.2        | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.9.1            | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.12.1      | 2.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "harfbuzz-12.2.0      | 2.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sdl3-3.2.26          | 1.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-onnx-fro | 1.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-intel-cp | 11.8 MB   | :  17% 0.17340384414026497/1 [00:00<00:00,  1.73it/s]\n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | :   2% 0.02272298594237149/1 [00:00<00:04,  4.40s/it]\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | :   0% 0.00135078949540213/1 [00:00<01:16, 76.57s/it]\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | :   8% 0.07614771565674151/1 [00:00<00:01,  1.32s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-intel-cp | 11.8 MB   | :  54% 0.5400669344215886/1 [00:00<00:00,  2.87it/s] \n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | :  51% 0.5074800193796299/1 [00:00<00:00,  2.94it/s] \u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | :  42% 0.4241479015562688/1 [00:00<00:00,  2.45it/s] \u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | :  46% 0.46154839898065775/1 [00:00<00:00,  2.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-intel-cp | 11.8 MB   | :  98% 0.9795331653724891/1 [00:00<00:00,  3.56it/s]\n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | :  94% 0.9437613494731626/1 [00:00<00:00,  3.59it/s]\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | :  90% 0.8982750144424164/1 [00:00<00:00,  3.49it/s]\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | :  90% 0.9044483778004808/1 [00:00<00:00,  3.42it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "intel-media-driver-2 | 8.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.63it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | : 100% 1.0/1 [00:00<00:00,  3.49it/s]               \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-intel-cp | 11.8 MB   | : 100% 1.0/1 [00:00<00:00,  3.56it/s]               \n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | : 100% 1.0/1 [00:00<00:00,  3.42it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | : 100% 1.0/1 [00:00<00:00,  3.59it/s]               \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-6.31.1   | 4.4 MB    | :   0% 0.003526568509361851/1 [00:00<02:00, 120.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "librsvg-2.60.0       | 3.3 MB    | :   0% 0.004787875546796486/1 [00:00<01:32, 93.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.86.1       | 3.8 MB    | :   0% 0.0041650280511487/1 [00:00<01:49, 109.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gettext-tools-0.25.1 | 3.5 MB    | :   0% 0.00449603098485416/1 [00:00<01:41, 101.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-2025.2.0 | 6.0 MB    | :  76% 0.7556068909492438/1 [00:00<00:00,  2.06it/s]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-6.31.1   | 4.4 MB    | :  80% 0.8040576201345021/1 [00:00<00:00,  2.01it/s]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "librsvg-2.60.0       | 3.3 MB    | :  98% 0.9767266115464832/1 [00:00<00:00,  2.37it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.86.1       | 3.8 MB    | :  88% 0.8788209187923758/1 [00:00<00:00,  2.09it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gettext-tools-0.25.1 | 3.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.17it/s]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gettext-tools-0.25.1 | 3.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "librsvg-2.60.0       | 3.3 MB    | : 100% 1.0/1 [00:00<00:00,  2.37it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.86.1       | 3.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.09it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-6.31.1   | 4.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.01it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-2025.2.0 | 6.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.06it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | :   1% 0.005175966758061148/1 [00:00<02:02, 123.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | :   0% 0.004880274801411181/1 [00:00<02:14, 135.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.9.1            | 2.6 MB    | :   1% 0.006053807351178468/1 [00:00<01:49, 110.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-3.1.2        | 2.6 MB    | :   1% 0.005976944403910696/1 [00:00<01:54, 114.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.12.1      | 2.3 MB    | :   1% 0.006685595039993602/1 [00:00<01:47, 108.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00, 123.00s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "harfbuzz-12.2.0      | 2.3 MB    | :   1% 0.006794370757665231/1 [00:00<01:52, 113.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.9.1            | 2.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.69it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.9.1            | 2.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.66it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.63it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.12.1      | 2.3 MB    | : 100% 1.0/1 [00:00<00:00, 108.10s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "harfbuzz-12.2.0      | 2.3 MB    | : 100% 1.0/1 [00:00<00:00, 113.29s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sdl3-3.2.26          | 1.8 MB    | :   1% 0.008461446295844884/1 [00:00<01:39, 99.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-onnx-fro | 1.6 MB    | :   1% 0.00950070832002032/1 [00:00<01:28, 89.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "intel-media-driver-2 | 8.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | :   1% 0.010110434778315882/1 [00:00<01:26, 86.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:00<00:00, 89.38s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sdl3-3.2.26          | 1.8 MB    | : 100% 1.0/1 [00:00<00:00, 99.99s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-intel-cp | 11.8 MB   | : 100% 1.0/1 [00:01<00:00,  3.56it/s]\n",
            "icu-75.1             | 11.6 MB   | : 100% 1.0/1 [00:01<00:00,  3.49it/s]\u001b[A\n",
            "\n",
            "libopenvino-intel-gp | 10.3 MB   | : 100% 1.0/1 [00:01<00:00,  3.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "librsvg-2.60.0       | 3.3 MB    | : 100% 1.0/1 [00:01<00:00,  2.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ffmpeg-7.1.1         | 10.1 MB   | : 100% 1.0/1 [00:01<00:00,  3.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gettext-tools-0.25.1 | 3.5 MB    | : 100% 1.0/1 [00:01<00:00,  2.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.86.1       | 3.8 MB    | : 100% 1.0/1 [00:02<00:00,  2.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-2025.2.0 | 6.0 MB    | : 100% 1.0/1 [00:02<00:00,  2.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-6.31.1   | 4.4 MB    | : 100% 1.0/1 [00:02<00:00,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aom-3.9.1            | 2.6 MB    | : 100% 1.0/1 [00:02<00:00,  1.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  2.05s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.6.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  2.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:02<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "harfbuzz-12.2.0      | 2.3 MB    | : 100% 1.0/1 [00:02<00:00,  2.18s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "harfbuzz-12.2.0      | 2.3 MB    | : 100% 1.0/1 [00:02<00:00,  2.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.12.1      | 2.3 MB    | : 100% 1.0/1 [00:02<00:00,  2.36s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.12.1      | 2.3 MB    | : 100% 1.0/1 [00:02<00:00,  2.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:02<00:00,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:02<00:00,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sdl3-3.2.26          | 1.8 MB    | : 100% 1.0/1 [00:02<00:00,  2.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sdl3-3.2.26          | 1.8 MB    | : 100% 1.0/1 [00:02<00:00,  2.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:02<00:00,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:02<00:00,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:03<00:00,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \n",
            "\b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \n",
            "\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Obtaining file:///content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets<4.2.0,>=4.0.0 (from lerobot==0.4.2)\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting diffusers<0.36.0,>=0.27.2 (from lerobot==0.4.2)\n",
            "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting huggingface-hub<0.36.0,>=0.34.2 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting accelerate<2.0.0,>=1.10.0 (from lerobot==0.4.2)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting setuptools<81.0.0,>=71.0.0 (from lerobot==0.4.2)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting cmake<4.2.0,>=3.29.0.1 (from lerobot==0.4.2)\n",
            "  Downloading cmake-4.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting einops<0.9.0,>=0.8.0 (from lerobot==0.4.2)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting opencv-python-headless<4.13.0,>=4.9.0 (from lerobot==0.4.2)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting av<16.0.0,>=15.0.0 (from lerobot==0.4.2)\n",
            "  Downloading av-15.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines<5.0.0,>=4.0.0 (from lerobot==0.4.2)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging<26.0,>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (24.2)\n",
            "Collecting pynput<1.9.0,>=1.7.7 (from lerobot==0.4.2)\n",
            "  Downloading pynput-1.8.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting pyserial<4.0,>=3.5 (from lerobot==0.4.2)\n",
            "  Downloading pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting wandb<0.22.0,>=0.20.0 (from lerobot==0.4.2)\n",
            "  Downloading wandb-0.21.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting torch<2.8.0,>=2.2.1 (from lerobot==0.4.2)\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchcodec<0.6.0,>=0.2.1 (from lerobot==0.4.2)\n",
            "  Downloading torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting torchvision<0.23.0,>=0.21.0 (from lerobot==0.4.2)\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting draccus==0.10.0 (from lerobot==0.4.2)\n",
            "  Downloading draccus-0.10.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting gymnasium<2.0.0,>=1.1.1 (from lerobot==0.4.2)\n",
            "  Downloading gymnasium-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rerun-sdk<0.27.0,>=0.24.0 (from lerobot==0.4.2)\n",
            "  Downloading rerun_sdk-0.26.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting deepdiff<9.0.0,>=7.0.1 (from lerobot==0.4.2)\n",
            "  Downloading deepdiff-8.6.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting imageio<3.0.0,>=2.34.0 (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.2)\n",
            "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting termcolor<4.0.0,>=2.4.0 (from lerobot==0.4.2)\n",
            "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting mergedeep~=1.3 (from draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pyyaml~=6.0 (from draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pyyaml-include~=1.4 (from draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading pyyaml_include-1.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting toml~=0.10 (from draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect~=0.9.0 (from draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting numpy>=1.17 (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2)\n",
            "  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting psutil (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2)\n",
            "  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting safetensors>=0.4.3 (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting filelock (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (4.67.1)\n",
            "Collecting xxhash (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting orderly-set<6,>=5.4.1 (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.2)\n",
            "  Downloading orderly_set-5.5.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting importlib_metadata (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting regex!=2019.12.17 (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2)\n",
            "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting Pillow (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2)\n",
            "  Downloading pillow-12.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2)\n",
            "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions>=4.3.0 (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting prompt-toolkit<4.0.0,>=3.0.1 (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting imageio-ffmpeg (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.2)\n",
            "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting attrs>=19.2.0 (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy>=1.17 (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting six (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting evdev>=1.3 (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2)\n",
            "  Downloading evdev-1.9.2.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-xlib>=0.17 (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2)\n",
            "  Downloading python_xlib-0.33-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting sympy>=1.13.3 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting click>=8.0.1 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (4.3.6)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pydantic<3 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading sentry_sdk-2.45.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2025.11.12)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.2)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting zipp>=3.20 (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.2)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.2)\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2)\n",
            "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.2)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2)\n",
            "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading draccus-0.10.0-py3-none-any.whl (71 kB)\n",
            "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "Downloading av-15.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.6/39.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmake-4.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (29.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "Downloading deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
            "Downloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading gymnasium-1.2.2-py3-none-any.whl (952 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynput-1.8.1-py2.py3-none-any.whl (91 kB)\n",
            "Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "Downloading rerun_sdk-0.26.2-cp39-abi3-manylinux_2_28_x86_64.whl (98.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m151.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.21.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderly_set-5.5.0-py3-none-any.whl (13 kB)\n",
            "Downloading pillow-12.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_xlib-0.33-py2.py3-none-any.whl (182 kB)\n",
            "Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml_include-1.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading sentry_sdk-2.45.0-py2.py3-none-any.whl (404 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m170.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m203.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
            "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
            "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
            "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: lerobot, evdev\n",
            "  Building editable for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.4.2-0.editable-py3-none-any.whl size=15629 sha256=7d55e4f4888c6a3036699b24fdef54281c6f6a0f342062aaf80644997bfd1391\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bk9idhef/wheels/15/0d/02/b9c6ff1c78574dee99101ad231194b3425eb4cd784ce8c8338\n",
            "  Building wheel for evdev (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evdev: filename=evdev-1.9.2-cp311-cp311-linux_x86_64.whl size=74951 sha256=2668555218892d1613a9eb0009e5d3a7e6ea69210b9244df6e142fef69e8e464\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/97/d0/ea1b02915719d1cdb6a8810aa7683524c7aceedc5812cdeed7\n",
            "Successfully built lerobot evdev\n",
            "Installing collected packages: pytz, pyserial, nvidia-cusparselt-cu12, mpmath, farama-notifications, zipp, xxhash, wcwidth, tzdata, typing-extensions, torchcodec, toml, termcolor, sympy, smmap, six, setuptools, sentry-sdk, safetensors, regex, pyyaml, pyarrow, psutil, protobuf, propcache, Pillow, pfzy, orderly-set, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mypy-extensions, multidict, mergedeep, MarkupSafe, imageio-ffmpeg, hf-xet, hf-transfer, fsspec, frozenlist, filelock, evdev, einops, dill, cmake, cloudpickle, click, av, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, triton, rerun-sdk, pyyaml-include, python-xlib, python-dateutil, pydantic-core, prompt-toolkit, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jsonlines, jinja2, importlib_metadata, imageio, huggingface-hub, gymnasium, gitdb, deepdiff, aiosignal, pynput, pydantic, pandas, nvidia-cusolver-cu12, InquirerPy, gitpython, draccus, diffusers, aiohttp, wandb, torch, torchvision, datasets, accelerate, lerobot\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 65.6.3\n",
            "    Uninstalling setuptools-65.6.3:\n",
            "      Successfully uninstalled setuptools-65.6.3\n",
            "Successfully installed InquirerPy-0.3.4 MarkupSafe-3.0.3 Pillow-12.0.0 accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 attrs-25.4.0 av-15.1.0 click-8.3.1 cloudpickle-3.1.2 cmake-4.1.2 datasets-4.1.1 deepdiff-8.6.1 diffusers-0.35.2 dill-0.4.0 draccus-0.10.0 einops-0.8.1 evdev-1.9.2 farama-notifications-0.0.4 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 gitdb-4.0.12 gitpython-3.1.45 gymnasium-1.2.2 hf-transfer-0.1.9 hf-xet-1.2.0 huggingface-hub-0.35.3 imageio-2.37.2 imageio-ffmpeg-0.6.0 importlib_metadata-8.7.0 jinja2-3.1.6 jsonlines-4.0.0 lerobot-0.4.2 mergedeep-1.3.4 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.16 mypy-extensions-1.1.0 networkx-3.5 numpy-2.2.6 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-headless-4.12.0.88 orderly-set-5.5.0 pandas-2.3.3 pfzy-0.3.4 prompt-toolkit-3.0.52 propcache-0.4.1 protobuf-6.33.1 psutil-7.1.3 pyarrow-22.0.0 pydantic-2.12.4 pydantic-core-2.41.5 pynput-1.8.1 pyserial-3.5 python-dateutil-2.9.0.post0 python-xlib-0.33 pytz-2025.2 pyyaml-6.0.3 pyyaml-include-1.4.1 regex-2025.11.3 rerun-sdk-0.26.2 safetensors-0.6.2 sentry-sdk-2.45.0 setuptools-80.9.0 six-1.17.0 smmap-5.0.2 sympy-1.14.0 termcolor-3.2.0 toml-0.10.2 torch-2.7.1 torchcodec-0.5 torchvision-0.22.1 triton-3.3.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 tzdata-2025.2 wandb-0.21.4 wcwidth-0.2.14 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge\n",
        "!cd lerobot && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Sn2wG4wldo"
      },
      "source": [
        "## Weights & Biases login\n",
        "This cell logs you into Weights & Biases (wandb) to enable experiment tracking and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PolVM_movEvp",
        "outputId": "4973a477-ef6b-47ee-eaf1-f2646a2fc98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malanc-za\u001b[0m (\u001b[33malanc-za-university-of-pretoria\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2ipRyNfiGlV"
      },
      "source": [
        "## Install SmolVLA dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oFA08IoWiGlV",
        "outputId": "04b8f8c2-4785-48b7-a6ac-dffcecbe5a7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets<4.2.0,>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (4.1.1)\n",
            "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.35.2)\n",
            "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (0.35.3)\n",
            "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (1.11.0)\n",
            "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (80.9.0)\n",
            "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (4.1.2)\n",
            "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.8.1)\n",
            "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (4.12.0.88)\n",
            "Requirement already satisfied: av<16.0.0,>=15.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (15.1.0)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (4.0.0)\n",
            "Requirement already satisfied: packaging<26.0,>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (24.2)\n",
            "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (1.8.1)\n",
            "Requirement already satisfied: pyserial<4.0,>=3.5 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (3.5)\n",
            "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.21.4)\n",
            "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (2.7.1)\n",
            "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.5)\n",
            "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.22.1)\n",
            "Requirement already satisfied: draccus==0.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.10.0)\n",
            "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (1.2.2)\n",
            "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.26.2)\n",
            "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (8.6.1)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.2) (2.37.2)\n",
            "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (3.2.0)\n",
            "Requirement already satisfied: mergedeep~=1.3 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.4.2) (1.3.4)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.4.2) (6.0.3)\n",
            "Requirement already satisfied: pyyaml-include~=1.4 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: toml~=0.10 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.4.2) (0.10.2)\n",
            "Requirement already satisfied: typing-inspect~=0.9.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.4.2) (0.9.0)\n",
            "Collecting num2words<0.6.0,>=0.5.14 (from lerobot==0.4.2)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: safetensors<1.0.0,>=0.4.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.4.2) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2) (2.2.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.2) (7.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2025.9.0)\n",
            "Requirement already satisfied: orderly-set<6,>=5.4.1 in /usr/local/lib/python3.11/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.2) (5.5.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2) (8.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2) (2025.11.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.2) (12.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.2) (0.0.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (1.2.0)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (0.3.4)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (0.1.9)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (3.0.52)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.2) (0.6.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.2) (25.4.0)\n",
            "Collecting docopt>=0.6.2 (from num2words<0.6.0,>=0.5.14->lerobot==0.4.2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2) (1.17.0)\n",
            "Requirement already satisfied: evdev>=1.3 in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2) (1.9.2)\n",
            "Requirement already satisfied: python-xlib>=0.17 in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.2) (0.33)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.2) (3.3.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (6.33.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (2.12.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (2.45.0)\n",
            "Collecting transformers<5.0.0,>=4.53.0 (from lerobot==0.4.2)\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.2) (1.3.0)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.53.0->lerobot==0.4.2)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.2) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.2) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.2) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.2) (5.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.2) (0.2.14)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m173.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lerobot, docopt\n",
            "  Building editable for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.4.2-0.editable-py3-none-any.whl size=15629 sha256=e5dcea08a4e6e7404d3152b684e846ddfacc8d31a4075635f27843b14a4df594\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-85fdrqf4/wheels/15/0d/02/b9c6ff1c78574dee99101ad231194b3425eb4cd784ce8c8338\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=8f33ace0041b959518f60f14061e3af6cea7db0c0a2d7abe53d5c7e247a78caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built lerobot docopt\n",
            "Installing collected packages: docopt, num2words, tokenizers, transformers, lerobot\n",
            "  Attempting uninstall: lerobot\n",
            "    Found existing installation: lerobot 0.4.2\n",
            "    Uninstalling lerobot-0.4.2:\n",
            "      Successfully uninstalled lerobot-0.4.2\n",
            "Successfully installed docopt-0.6.2 lerobot-0.4.2 num2words-0.5.14 tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && pip install -e \".[smolvla]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkzTo4mNwxaC"
      },
      "source": [
        "## Start training SmolVLA with LeRobot\n",
        "\n",
        "This cell runs the `train.py` script from the `lerobot` library to train a robot control policy.  \n",
        "\n",
        "Make sure to adjust the following arguments to your setup:\n",
        "\n",
        "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
        "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `pepijn223/il_gym0`.\n",
        "\n",
        "2. `--batch_size=64`: means the model processes 64 training samples in parallel before doing one gradient update. Reduce this number if you have a GPU with low memory.\n",
        "\n",
        "3. `--output_dir=outputs/train/...`:  \n",
        "   Directory where training logs and model checkpoints will be saved.\n",
        "\n",
        "4. `--job_name=...`:  \n",
        "   A name for this training job, used for logging and Weights & Biases.\n",
        "\n",
        "5. `--policy.device=cuda`:  \n",
        "   Use `cuda` if training on an NVIDIA GPU. Use `mps` for Apple Silicon, or `cpu` if no GPU is available.\n",
        "\n",
        "6. `--wandb.enable=true`:  \n",
        "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g7ZVVZPkiGlV",
        "outputId": "fa021a3d-1203-4575-8d3d-c3ce681ba264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 2025-11-19 14:44:16 ot_train.py:163 {'batch_size': 64,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'Alkatt/so101_CubeToBowl_PickPlace_ASN_V2',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'finetine_smolvla_so101_CubeToBowl_ASN_V5',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': 'outputs/train/smolvla_so101_CubeToBowl_ASN_Finetuned_V5',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Alkatt/smolvla_so101_CubeToBowl_ASN_Finetuned_V5',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 20000,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': True,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "INFO 2025-11-19 14:44:18 db_utils.py:102 \u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
            "INFO 2025-11-19 14:44:18 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/alanc-za-university-of-pretoria/lerobot/runs/4z143swl\u001b[0m\n",
            "INFO 2025-11-19 14:44:18 ot_train.py:183 Creating dataset\n",
            "Fetching 8 files:   0% 0/8 [00:00<?, ?it/s]\n",
            "meta/episodes/chunk-000/file-003.parquet:   0% 0.00/61.1k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "meta/episodes/chunk-000/file-000.parquet:   0% 0.00/298k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-004.parquet:   0% 0.00/76.1k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "meta/tasks.parquet:   0% 0.00/2.18k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-001.parquet:   0% 0.00/69.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-002.parquet:   0% 0.00/305k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "info.json: 3.87kB [00:00, 9.63MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "stats.json: 16.1kB [00:00, 39.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "meta/tasks.parquet: 100% 2.18k/2.18k [00:01<00:00, 1.16kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-002.parquet: 100% 305k/305k [00:02<00:00, 139kB/s]\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-000.parquet: 100% 298k/298k [00:02<00:00, 129kB/s]\n",
            "Fetching 8 files:  12% 1/8 [00:02<00:19,  2.82s/it]\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-004.parquet: 100% 76.1k/76.1k [00:02<00:00, 27.6kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "meta/episodes/chunk-000/file-001.parquet: 100% 69.6k/69.6k [00:02<00:00, 24.3kB/s]\n",
            "Fetching 8 files:  25% 2/8 [00:03<00:09,  1.50s/it]\n",
            "meta/episodes/chunk-000/file-003.parquet: 100% 61.1k/61.1k [00:03<00:00, 20.4kB/s]\n",
            "Fetching 8 files: 100% 8/8 [00:03<00:00,  2.29it/s]\n",
            "Fetching 30 files:   0% 0/30 [00:00<?, ?it/s]\n",
            "data/chunk-000/file-004.parquet:   0% 0.00/196k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/file-002.parquet:   0% 0.00/1.05M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/file-003.parquet:   0% 0.00/20.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/file-001.parquet:   0% 0.00/117k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/file-000.parquet:   0% 0.00/962k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 2.46kB [00:00, 3.54MB/s]\n",
            "Fetching 30 files:   3% 1/30 [00:00<00:21,  1.36it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md: 4.36kB [00:00, 10.3MB/s]\n",
            "\n",
            "\n",
            "data/chunk-000/file-002.parquet: 100% 1.05M/1.05M [00:00<00:00, 2.15MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/file-001.parquet: 100% 117k/117k [00:00<00:00, 233kB/s]\n",
            "\n",
            "data/chunk-000/file-004.parquet: 100% 196k/196k [00:00<00:00, 339kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/file-000.parquet: 100% 962k/962k [00:00<00:00, 1.92MB/s]\n",
            "Fetching 30 files:  10% 3/30 [00:01<00:08,  3.10it/s]\n",
            "videos/observation.images.camera1/chunk-(â€¦):   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):   0% 0.00/200M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):   0% 0.00/21.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):   0% 0.00/3.55M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 0.00/19.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 0.00/169M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):   0% 0.00/36.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/file-003.parquet: 100% 20.2k/20.2k [00:02<00:00, 9.92kB/s]\n",
            "Fetching 30 files:  20% 6/30 [00:02<00:10,  2.30it/s]\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 0.00/190M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 17.9k/190M [00:00<2:52:22, 18.4kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦): 100% 3.55M/3.55M [00:02<00:00, 1.38MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 0.00/3.37M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦): 100% 21.1M/21.1M [00:03<00:00, 6.16MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦): 100% 19.6M/19.6M [00:03<00:00, 5.63MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):   0% 0.00/35.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):   0% 0.00/81.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):  20% 34.4M/169M [00:04<00:17, 7.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/observation.images.camera1/chunk-(â€¦):  24% 41.7M/176M [00:04<00:15, 8.49MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦): 100% 36.7M/36.7M [00:04<00:00, 7.69MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):   0% 0.00/9.96M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦): 100% 3.37M/3.37M [00:02<00:00, 1.37MB/s]\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):  62% 109M/176M [00:06<00:03, 19.9MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦): 100% 176M/176M [00:06<00:00, 27.7MB/s]\n",
            "Fetching 30 files:  53% 16/30 [00:07<00:06,  2.11it/s]\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):  33% 67.1M/200M [00:06<00:13, 10.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦):  67% 133M/200M [00:06<00:02, 23.4MB/s] \u001b[A\u001b[A\n",
            "videos/observation.images.camera3/chunk-(â€¦):   0% 0.00/1.59M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):  60% 101M/169M [00:06<00:03, 17.5MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):   0% 17.9k/91.4M [00:00<1:14:33, 20.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/observation.images.camera1/chunk-(â€¦): 100% 200M/200M [00:07<00:00, 28.0MB/s]\n",
            "Fetching 30 files:  60% 18/30 [00:08<00:05,  2.14it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦): 100% 169M/169M [00:06<00:00, 24.2MB/s]\n",
            "Fetching 30 files:  70% 21/30 [00:08<00:03,  2.81it/s]\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):  29% 55.8M/190M [00:05<00:13, 10.0MB/s]  \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):  18% 14.5M/81.6M [00:03<00:15, 4.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):   0% 0.00/16.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦): 100% 35.6M/35.6M [00:04<00:00, 8.43MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦):  65% 123M/190M [00:06<00:02, 23.7MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/observation.images.camera2/chunk-(â€¦): 100% 190M/190M [00:06<00:00, 28.9MB/s]\n",
            "Fetching 30 files:  77% 23/30 [00:09<00:02,  2.52it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦): 100% 9.96M/9.96M [00:02<00:00, 3.39MB/s]\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦): 100% 1.59M/1.59M [00:02<00:00, 723kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦):  27% 24.4M/91.4M [00:03<00:10, 6.62MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦): 100% 81.6M/81.6M [00:06<00:00, 13.4MB/s]\n",
            "Fetching 30 files:  87% 26/30 [00:11<00:01,  2.11it/s]\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦): 100% 16.0M/16.0M [00:03<00:00, 5.00MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/observation.images.camera3/chunk-(â€¦): 100% 91.4M/91.4M [00:06<00:00, 14.4MB/s]\n",
            "Fetching 30 files: 100% 30/30 [00:13<00:00,  2.17it/s]\n",
            "INFO 2025-11-19 14:44:37 ot_train.py:202 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "config.json: 3.77kB [00:00, 6.95MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 2.03G/2.03G [00:03<00:00, 562MB/s]\n",
            "INFO 2025-11-19 14:44:41 modeling.py:987 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "generation_config.json: 100% 136/136 [00:00<00:00, 836kB/s]\n",
            "processor_config.json: 100% 67.0/67.0 [00:00<00:00, 428kB/s]\n",
            "chat_template.json: 100% 430/430 [00:00<00:00, 1.91MB/s]\n",
            "preprocessor_config.json: 100% 599/599 [00:00<00:00, 3.38MB/s]\n",
            "tokenizer_config.json: 28.6kB [00:00, 44.1MB/s]\n",
            "vocab.json: 801kB [00:00, 96.0MB/s]\n",
            "merges.txt: 466kB [00:00, 112MB/s]\n",
            "tokenizer.json: 3.55MB [00:00, 205MB/s]\n",
            "added_tokens.json: 4.74kB [00:00, 7.96MB/s]\n",
            "special_tokens_map.json: 100% 868/868 [00:00<00:00, 5.11MB/s]\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "model.safetensors: 100% 907M/907M [00:01<00:00, 518MB/s] \n",
            "policy_preprocessor.json: 1.87kB [00:00, 4.19MB/s]\n",
            "policy_preprocessor_step_5_normalizer_pr(â€¦): 100% 640/640 [00:00<00:00, 1.29kB/s]\n",
            "policy_postprocessor.json: 100% 660/660 [00:00<00:00, 3.49MB/s]\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:247 Creating optimizer and scheduler\n",
            "INFO 2025-11-19 14:45:02 hedulers.py:105 Auto-scaling LR scheduler: num_training_steps (20000) < num_decay_steps (30000). Scaling warmup: 1000 â†’ 666, decay: 30000 â†’ 20000 (scale factor: 0.667)\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/smolvla_so101_CubeToBowl_ASN_Finetuned_V5\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:262 cfg.steps=20000 (20K)\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:263 dataset.num_frames=49287 (49K)\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:264 dataset.num_episodes=130\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:267 Effective batch size: 64 x 1 = 64\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:268 num_learnable_params=99880992 (100M)\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:269 num_total_params=450046176 (450M)\n",
            "INFO 2025-11-19 14:45:02 ot_train.py:325 Start offline training on a fixed dataset\n",
            "INFO 2025-11-19 14:48:09 ot_train.py:352 step:200 smpl:13K ep:34 epch:0.26 loss:0.067 grdn:0.530 lr:1.5e-05 updt_s:0.851 data_s:0.086\n",
            "WARNING 2025-11-19 14:48:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 14:48:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 14:51:10 ot_train.py:352 step:400 smpl:26K ep:68 epch:0.52 loss:0.037 grdn:0.434 lr:4.5e-05 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 14:51:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 14:51:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 14:54:11 ot_train.py:352 step:600 smpl:38K ep:101 epch:0.78 loss:0.033 grdn:0.439 lr:7.5e-05 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 14:54:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 14:54:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 14:57:15 ot_train.py:352 step:800 smpl:51K ep:135 epch:1.04 loss:0.034 grdn:0.452 lr:9.8e-05 updt_s:0.836 data_s:0.081\n",
            "WARNING 2025-11-19 14:57:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 14:57:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:00:16 ot_train.py:352 step:1K smpl:64K ep:169 epch:1.30 loss:0.032 grdn:0.397 lr:1.0e-04 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 15:00:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:00:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:03:17 ot_train.py:352 step:1K smpl:77K ep:203 epch:1.56 loss:0.030 grdn:0.369 lr:9.9e-05 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 15:03:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:03:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:06:18 ot_train.py:352 step:1K smpl:90K ep:236 epch:1.82 loss:0.029 grdn:0.351 lr:9.9e-05 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 15:06:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:06:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 15:09:22 ot_train.py:352 step:2K smpl:102K ep:270 epch:2.08 loss:0.028 grdn:0.347 lr:9.9e-05 updt_s:0.835 data_s:0.083\n",
            "WARNING 2025-11-19 15:09:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:09:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:12:23 ot_train.py:352 step:2K smpl:115K ep:304 epch:2.34 loss:0.027 grdn:0.327 lr:9.8e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:12:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:12:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:15:24 ot_train.py:352 step:2K smpl:128K ep:338 epch:2.60 loss:0.026 grdn:0.315 lr:9.8e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:15:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:15:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:18:25 ot_train.py:352 step:2K smpl:141K ep:371 epch:2.86 loss:0.026 grdn:0.311 lr:9.7e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:18:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:18:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 15:21:30 ot_train.py:352 step:2K smpl:154K ep:405 epch:3.12 loss:0.025 grdn:0.301 lr:9.7e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 15:21:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:21:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:24:31 ot_train.py:352 step:3K smpl:166K ep:439 epch:3.38 loss:0.024 grdn:0.287 lr:9.6e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:24:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:24:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:27:32 ot_train.py:352 step:3K smpl:179K ep:473 epch:3.64 loss:0.024 grdn:0.276 lr:9.6e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:27:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:27:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:30:33 ot_train.py:352 step:3K smpl:192K ep:506 epch:3.90 loss:0.024 grdn:0.291 lr:9.5e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:30:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:30:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 15:33:37 ot_train.py:352 step:3K smpl:205K ep:540 epch:4.16 loss:0.023 grdn:0.271 lr:9.4e-05 updt_s:0.837 data_s:0.082\n",
            "WARNING 2025-11-19 15:33:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:33:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:36:38 ot_train.py:352 step:3K smpl:218K ep:574 epch:4.41 loss:0.022 grdn:0.271 lr:9.4e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:36:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:36:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:39:39 ot_train.py:352 step:4K smpl:230K ep:608 epch:4.67 loss:0.023 grdn:0.260 lr:9.3e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:39:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:39:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:42:40 ot_train.py:352 step:4K smpl:243K ep:641 epch:4.93 loss:0.022 grdn:0.247 lr:9.2e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 15:42:40 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:42:40 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 15:45:45 ot_train.py:352 step:4K smpl:256K ep:675 epch:5.19 loss:0.022 grdn:0.254 lr:9.1e-05 updt_s:0.836 data_s:0.084\n",
            "WARNING 2025-11-19 15:45:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:45:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:48:46 ot_train.py:352 step:4K smpl:269K ep:709 epch:5.45 loss:0.021 grdn:0.242 lr:9.0e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:48:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:48:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:51:47 ot_train.py:352 step:4K smpl:282K ep:743 epch:5.71 loss:0.021 grdn:0.249 lr:8.9e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:51:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:51:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 15:54:49 ot_train.py:352 step:5K smpl:294K ep:777 epch:5.97 loss:0.021 grdn:0.239 lr:8.8e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 15:54:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:54:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 15:57:53 ot_train.py:352 step:5K smpl:307K ep:810 epch:6.23 loss:0.021 grdn:0.245 lr:8.7e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 15:57:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 15:57:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:00:54 ot_train.py:352 step:5K smpl:320K ep:844 epch:6.49 loss:0.020 grdn:0.229 lr:8.6e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:00:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:00:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:03:56 ot_train.py:352 step:5K smpl:333K ep:878 epch:6.75 loss:0.019 grdn:0.219 lr:8.5e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:03:56 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:03:56 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 16:07:00 ot_train.py:352 step:5K smpl:346K ep:912 epch:7.01 loss:0.020 grdn:0.225 lr:8.4e-05 updt_s:0.837 data_s:0.084\n",
            "WARNING 2025-11-19 16:07:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:07:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:10:01 ot_train.py:352 step:6K smpl:358K ep:945 epch:7.27 loss:0.019 grdn:0.221 lr:8.3e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:10:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:10:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:13:03 ot_train.py:352 step:6K smpl:371K ep:979 epch:7.53 loss:0.018 grdn:0.216 lr:8.2e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:13:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:13:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:16:04 ot_train.py:352 step:6K smpl:384K ep:1K epch:7.79 loss:0.019 grdn:0.223 lr:8.1e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:16:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:16:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 16:19:08 ot_train.py:352 step:6K smpl:397K ep:1K epch:8.05 loss:0.018 grdn:0.218 lr:7.9e-05 updt_s:0.837 data_s:0.083\n",
            "WARNING 2025-11-19 16:19:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:19:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:22:10 ot_train.py:352 step:6K smpl:410K ep:1K epch:8.31 loss:0.018 grdn:0.210 lr:7.8e-05 updt_s:0.841 data_s:0.064\n",
            "WARNING 2025-11-19 16:22:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:22:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:25:11 ot_train.py:352 step:7K smpl:422K ep:1K epch:8.57 loss:0.018 grdn:0.221 lr:7.7e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 16:25:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:25:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:28:13 ot_train.py:352 step:7K smpl:435K ep:1K epch:8.83 loss:0.017 grdn:0.205 lr:7.5e-05 updt_s:0.841 data_s:0.064\n",
            "WARNING 2025-11-19 16:28:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:28:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 16:31:17 ot_train.py:352 step:7K smpl:448K ep:1K epch:9.09 loss:0.017 grdn:0.202 lr:7.4e-05 updt_s:0.837 data_s:0.084\n",
            "WARNING 2025-11-19 16:31:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:31:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:34:19 ot_train.py:352 step:7K smpl:461K ep:1K epch:9.35 loss:0.017 grdn:0.207 lr:7.3e-05 updt_s:0.841 data_s:0.065\n",
            "WARNING 2025-11-19 16:34:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:34:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:37:20 ot_train.py:352 step:7K smpl:474K ep:1K epch:9.61 loss:0.016 grdn:0.195 lr:7.1e-05 updt_s:0.841 data_s:0.065\n",
            "WARNING 2025-11-19 16:37:20 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:37:20 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:40:22 ot_train.py:352 step:8K smpl:486K ep:1K epch:9.87 loss:0.017 grdn:0.198 lr:7.0e-05 updt_s:0.841 data_s:0.064\n",
            "WARNING 2025-11-19 16:40:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:40:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 16:43:26 ot_train.py:352 step:8K smpl:499K ep:1K epch:10.13 loss:0.016 grdn:0.194 lr:6.8e-05 updt_s:0.837 data_s:0.084\n",
            "WARNING 2025-11-19 16:43:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:43:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:46:28 ot_train.py:352 step:8K smpl:512K ep:1K epch:10.39 loss:0.016 grdn:0.195 lr:6.7e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 16:46:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:46:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:49:29 ot_train.py:352 step:8K smpl:525K ep:1K epch:10.65 loss:0.016 grdn:0.184 lr:6.6e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 16:49:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:49:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:52:30 ot_train.py:352 step:8K smpl:538K ep:1K epch:10.91 loss:0.016 grdn:0.183 lr:6.4e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 16:52:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:52:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 16:55:34 ot_train.py:352 step:9K smpl:550K ep:1K epch:11.17 loss:0.016 grdn:0.193 lr:6.3e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 16:55:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:55:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 16:58:36 ot_train.py:352 step:9K smpl:563K ep:1K epch:11.43 loss:0.015 grdn:0.181 lr:6.1e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 16:58:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 16:58:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:01:37 ot_train.py:352 step:9K smpl:576K ep:2K epch:11.69 loss:0.015 grdn:0.179 lr:6.0e-05 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 17:01:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:01:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:04:38 ot_train.py:352 step:9K smpl:589K ep:2K epch:11.95 loss:0.015 grdn:0.183 lr:5.8e-05 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 17:04:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:04:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 17:07:42 ot_train.py:352 step:9K smpl:602K ep:2K epch:12.21 loss:0.015 grdn:0.180 lr:5.7e-05 updt_s:0.836 data_s:0.084\n",
            "WARNING 2025-11-19 17:07:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:07:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:10:43 ot_train.py:352 step:10K smpl:614K ep:2K epch:12.47 loss:0.014 grdn:0.178 lr:5.5e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 17:10:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:10:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:13:44 ot_train.py:352 step:10K smpl:627K ep:2K epch:12.73 loss:0.014 grdn:0.177 lr:5.4e-05 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 17:13:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:13:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:16:46 ot_train.py:352 step:10K smpl:640K ep:2K epch:12.99 loss:0.014 grdn:0.171 lr:5.2e-05 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 17:16:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:16:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 17:19:50 ot_train.py:352 step:10K smpl:653K ep:2K epch:13.24 loss:0.014 grdn:0.167 lr:5.0e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 17:19:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:19:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:22:51 ot_train.py:352 step:10K smpl:666K ep:2K epch:13.50 loss:0.014 grdn:0.160 lr:4.9e-05 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 17:22:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:22:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:25:52 ot_train.py:352 step:11K smpl:678K ep:2K epch:13.76 loss:0.014 grdn:0.167 lr:4.7e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 17:25:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:25:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 17:28:57 ot_train.py:352 step:11K smpl:691K ep:2K epch:14.02 loss:0.014 grdn:0.163 lr:4.6e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 17:28:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:28:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:31:58 ot_train.py:352 step:11K smpl:704K ep:2K epch:14.28 loss:0.013 grdn:0.156 lr:4.4e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 17:31:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:31:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:34:59 ot_train.py:352 step:11K smpl:717K ep:2K epch:14.54 loss:0.013 grdn:0.158 lr:4.3e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 17:34:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:34:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:38:00 ot_train.py:352 step:11K smpl:730K ep:2K epch:14.80 loss:0.013 grdn:0.157 lr:4.1e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 17:38:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:38:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 17:41:04 ot_train.py:352 step:12K smpl:742K ep:2K epch:15.06 loss:0.013 grdn:0.155 lr:4.0e-05 updt_s:0.837 data_s:0.081\n",
            "WARNING 2025-11-19 17:41:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:41:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:44:06 ot_train.py:352 step:12K smpl:755K ep:2K epch:15.32 loss:0.013 grdn:0.146 lr:3.8e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 17:44:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:44:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:47:07 ot_train.py:352 step:12K smpl:768K ep:2K epch:15.58 loss:0.013 grdn:0.152 lr:3.7e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 17:47:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:47:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:50:08 ot_train.py:352 step:12K smpl:781K ep:2K epch:15.84 loss:0.012 grdn:0.145 lr:3.5e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 17:50:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:50:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 17:53:13 ot_train.py:352 step:12K smpl:794K ep:2K epch:16.10 loss:0.012 grdn:0.147 lr:3.4e-05 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 17:53:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:53:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:56:13 ot_train.py:352 step:13K smpl:806K ep:2K epch:16.36 loss:0.012 grdn:0.144 lr:3.3e-05 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 17:56:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:56:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 17:59:15 ot_train.py:352 step:13K smpl:819K ep:2K epch:16.62 loss:0.012 grdn:0.146 lr:3.1e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 17:59:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 17:59:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:02:16 ot_train.py:352 step:13K smpl:832K ep:2K epch:16.88 loss:0.012 grdn:0.142 lr:3.0e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 18:02:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:02:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 18:05:20 ot_train.py:352 step:13K smpl:845K ep:2K epch:17.14 loss:0.012 grdn:0.141 lr:2.8e-05 updt_s:0.836 data_s:0.084\n",
            "WARNING 2025-11-19 18:05:20 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:05:20 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:08:21 ot_train.py:352 step:13K smpl:858K ep:2K epch:17.40 loss:0.012 grdn:0.137 lr:2.7e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:08:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:08:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:11:23 ot_train.py:352 step:14K smpl:870K ep:2K epch:17.66 loss:0.012 grdn:0.140 lr:2.6e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:11:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:11:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:14:24 ot_train.py:352 step:14K smpl:883K ep:2K epch:17.92 loss:0.012 grdn:0.138 lr:2.4e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:14:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:14:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 18:17:28 ot_train.py:352 step:14K smpl:896K ep:2K epch:18.18 loss:0.012 grdn:0.136 lr:2.3e-05 updt_s:0.836 data_s:0.084\n",
            "WARNING 2025-11-19 18:17:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:17:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:20:30 ot_train.py:352 step:14K smpl:909K ep:2K epch:18.44 loss:0.012 grdn:0.136 lr:2.2e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 18:20:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:20:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:23:31 ot_train.py:352 step:14K smpl:922K ep:2K epch:18.70 loss:0.012 grdn:0.134 lr:2.1e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 18:23:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:23:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:26:32 ot_train.py:352 step:15K smpl:934K ep:2K epch:18.96 loss:0.011 grdn:0.130 lr:2.0e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:26:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:26:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 18:29:36 ot_train.py:352 step:15K smpl:947K ep:2K epch:19.22 loss:0.011 grdn:0.133 lr:1.8e-05 updt_s:0.837 data_s:0.082\n",
            "WARNING 2025-11-19 18:29:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:29:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:32:37 ot_train.py:352 step:15K smpl:960K ep:3K epch:19.48 loss:0.011 grdn:0.127 lr:1.7e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:32:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:32:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:35:38 ot_train.py:352 step:15K smpl:973K ep:3K epch:19.74 loss:0.011 grdn:0.126 lr:1.6e-05 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 18:35:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:35:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:38:39 ot_train.py:352 step:15K smpl:986K ep:3K epch:20.00 loss:0.011 grdn:0.126 lr:1.5e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:38:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:38:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 18:41:44 ot_train.py:352 step:16K smpl:998K ep:3K epch:20.26 loss:0.011 grdn:0.125 lr:1.4e-05 updt_s:0.837 data_s:0.083\n",
            "WARNING 2025-11-19 18:41:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:41:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:44:45 ot_train.py:352 step:16K smpl:1M ep:3K epch:20.52 loss:0.011 grdn:0.124 lr:1.3e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:44:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:44:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:47:46 ot_train.py:352 step:16K smpl:1M ep:3K epch:20.78 loss:0.011 grdn:0.116 lr:1.2e-05 updt_s:0.840 data_s:0.065\n",
            "WARNING 2025-11-19 18:47:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:47:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 18:50:51 ot_train.py:352 step:16K smpl:1M ep:3K epch:21.04 loss:0.011 grdn:0.123 lr:1.1e-05 updt_s:0.837 data_s:0.085\n",
            "WARNING 2025-11-19 18:50:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:50:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:53:53 ot_train.py:352 step:16K smpl:1M ep:3K epch:21.30 loss:0.011 grdn:0.113 lr:1.1e-05 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:53:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:53:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:56:54 ot_train.py:352 step:17K smpl:1M ep:3K epch:21.56 loss:0.011 grdn:0.114 lr:9.7e-06 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:56:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:56:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 18:59:55 ot_train.py:352 step:17K smpl:1M ep:3K epch:21.82 loss:0.011 grdn:0.116 lr:8.9e-06 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 18:59:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 18:59:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 19:02:59 ot_train.py:352 step:17K smpl:1M ep:3K epch:22.07 loss:0.011 grdn:0.118 lr:8.2e-06 updt_s:0.837 data_s:0.082\n",
            "WARNING 2025-11-19 19:02:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:02:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:06:01 ot_train.py:352 step:17K smpl:1M ep:3K epch:22.33 loss:0.011 grdn:0.112 lr:7.5e-06 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 19:06:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:06:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:09:02 ot_train.py:352 step:17K smpl:1M ep:3K epch:22.59 loss:0.011 grdn:0.111 lr:6.8e-06 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 19:09:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:09:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:12:03 ot_train.py:352 step:18K smpl:1M ep:3K epch:22.85 loss:0.011 grdn:0.113 lr:6.2e-06 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 19:12:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:12:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 19:15:07 ot_train.py:352 step:18K smpl:1M ep:3K epch:23.11 loss:0.011 grdn:0.112 lr:5.6e-06 updt_s:0.837 data_s:0.082\n",
            "WARNING 2025-11-19 19:15:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:15:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:18:08 ot_train.py:352 step:18K smpl:1M ep:3K epch:23.37 loss:0.011 grdn:0.108 lr:5.1e-06 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 19:18:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:18:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:21:09 ot_train.py:352 step:18K smpl:1M ep:3K epch:23.63 loss:0.011 grdn:0.109 lr:4.7e-06 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 19:21:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:21:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:24:10 ot_train.py:352 step:18K smpl:1M ep:3K epch:23.89 loss:0.011 grdn:0.107 lr:4.2e-06 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 19:24:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:24:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 19:27:14 ot_train.py:352 step:19K smpl:1M ep:3K epch:24.15 loss:0.011 grdn:0.107 lr:3.8e-06 updt_s:0.836 data_s:0.083\n",
            "WARNING 2025-11-19 19:27:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:27:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:30:16 ot_train.py:352 step:19K smpl:1M ep:3K epch:24.41 loss:0.011 grdn:0.112 lr:3.5e-06 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 19:30:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:30:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:33:17 ot_train.py:352 step:19K smpl:1M ep:3K epch:24.67 loss:0.011 grdn:0.107 lr:3.2e-06 updt_s:0.838 data_s:0.064\n",
            "WARNING 2025-11-19 19:33:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:33:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:36:17 ot_train.py:352 step:19K smpl:1M ep:3K epch:24.93 loss:0.011 grdn:0.104 lr:3.0e-06 updt_s:0.839 data_s:0.064\n",
            "WARNING 2025-11-19 19:36:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:36:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-11-19 19:39:22 ot_train.py:352 step:19K smpl:1M ep:3K epch:25.19 loss:0.011 grdn:0.106 lr:2.8e-06 updt_s:0.836 data_s:0.084\n",
            "WARNING 2025-11-19 19:39:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:39:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:42:24 ot_train.py:352 step:20K smpl:1M ep:3K epch:25.45 loss:0.011 grdn:0.106 lr:2.7e-06 updt_s:0.841 data_s:0.065\n",
            "WARNING 2025-11-19 19:42:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:42:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:45:25 ot_train.py:352 step:20K smpl:1M ep:3K epch:25.71 loss:0.010 grdn:0.105 lr:2.6e-06 updt_s:0.839 data_s:0.065\n",
            "WARNING 2025-11-19 19:45:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:45:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:48:26 ot_train.py:352 step:20K smpl:1M ep:3K epch:25.97 loss:0.011 grdn:0.104 lr:2.5e-06 updt_s:0.840 data_s:0.064\n",
            "WARNING 2025-11-19 19:48:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-11-19 19:48:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-11-19 19:48:26 ot_train.py:362 Checkpoint policy after step 20000\n",
            "INFO 2025-11-19 19:48:32 ot_train.py:431 End of training\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 407, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lerobot-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/scripts/lerobot_train.py\", line 445, in main\n",
            "    train()\n",
            "  File \"/content/lerobot/src/lerobot/configs/parser.py\", line 233, in wrapper_inner\n",
            "    response = fn(cfg, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/scripts/lerobot_train.py\", line 435, in train\n",
            "    unwrapped_policy.push_model_to_hub(cfg)\n",
            "  File \"/content/lerobot/src/lerobot/policies/pretrained.py\", line 211, in push_model_to_hub\n",
            "    repo_id = api.create_repo(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/hf_api.py\", line 3766, in create_repo\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 480, in hf_raise_for_status\n",
            "    raise _format(HfHubHTTPError, str(e), response) from e\n",
            "huggingface_hub.errors.HfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-691e1f10-3ea7d0e9309d349d4f618a83;07ac86b4-5d6b-4c4b-a3c6-84fb7ab105f9)\n",
            "\n",
            "Invalid username or password.\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && lerobot-train \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --policy.repo_id=Alkatt/smolvla_so101_CubeToBowl_ASN_Finetuned_V5 \\\n",
        "  --dataset.repo_id=Alkatt/so101_CubeToBowl_PickPlace_ASN_V2 \\\n",
        "  --batch_size=64 \\\n",
        "  --steps=20000 \\\n",
        "  --output_dir=outputs/train/smolvla_so101_CubeToBowl_ASN_Finetuned_V5 \\\n",
        "  --job_name=finetine_smolvla_so101_CubeToBowl_ASN_V5 \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5CY_4SIiGlW"
      },
      "source": [
        "## Login into Hugging Face Hub\n",
        "Now after training is done login into the Hugging Face hub and upload the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yu5khQGIHi6",
        "outputId": "af0d15dc-908d-42fe-ad23-5dd5042c5106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mâš ï¸  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "The token `Google-Colab-HF-Token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Google-Colab-HF-Token`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFMLGuVkH7UN",
        "outputId": "f634b6bd-12a1-4a33-e580-2a01843b4a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mâš ï¸  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
            "Start hashing 7 files.\n",
            "Finished hashing 7 files.\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 609k/907M [00:02<49:43, 304kB/s,  338kB/s  ]\n",
            "New Data Upload               :   1% 609k/67.1M [00:02<03:38, 304kB/s,  338kB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 1.22M/907M [00:02<29:12, 517kB/s,  507kB/s  ]\n",
            "New Data Upload               :   1% 1.22M/134M [00:02<04:17, 517kB/s,  507kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 1.83M/907M [00:02<18:07, 832kB/s,  703kB/s  ]\n",
            "New Data Upload               :   1% 1.83M/134M [00:02<02:39, 832kB/s,  703kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 5.50M/907M [00:03<04:18, 3.48MB/s, 1.96MB/s  ]\n",
            "New Data Upload               :   4% 5.50M/134M [00:03<00:36, 3.48MB/s, 1.96MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:   1% 5.50M/907M [00:01<03:41, 4.07MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:   1% 5.50M/907M [00:01<04:18, 3.49MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 7.32M/907M [00:03<04:31, 3.31MB/s, 2.15MB/s  ]\n",
            "New Data Upload               :   4% 7.32M/201M [00:03<00:58, 3.31MB/s, 2.15MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 12.2M/907M [00:03<02:14, 6.65MB/s, 3.39MB/s  ]\n",
            "New Data Upload               :   6% 12.2M/201M [00:03<00:28, 6.65MB/s, 3.39MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   2% 17.8M/907M [00:04<01:24, 10.5MB/s, 4.67MB/s  ]\n",
            "New Data Upload               :   9% 17.8M/201M [00:04<00:17, 10.5MB/s, 4.67MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   3% 23.3M/907M [00:04<01:02, 14.1MB/s, 5.82MB/s  ]\n",
            "New Data Upload               :  12% 23.3M/201M [00:04<00:12, 14.1MB/s, 5.82MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   3% 27.6M/907M [00:04<00:55, 15.7MB/s, 6.56MB/s  ]\n",
            "New Data Upload               :  14% 27.6M/201M [00:04<00:11, 15.7MB/s, 6.56MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 35.6M/907M [00:04<00:40, 21.8MB/s, 8.09MB/s  ]\n",
            "New Data Upload               :  18% 35.6M/201M [00:04<00:07, 21.8MB/s, 8.09MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   5% 44.9M/907M [00:04<00:30, 28.2MB/s, 9.75MB/s  ]\n",
            "New Data Upload               :  22% 44.9M/201M [00:04<00:05, 28.2MB/s, 9.75MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   9% 78.6M/907M [00:05<00:12, 66.4MB/s, 16.4MB/s  ]\n",
            "New Data Upload               :  27% 53.5M/201M [00:05<00:04, 32.3MB/s, 11.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  14% 126M/907M [00:05<00:06, 114MB/s, 25.2MB/s  ]  \n",
            "New Data Upload               :  29% 59.1M/201M [00:05<00:04, 31.1MB/s, 11.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  18% 167M/907M [00:05<00:05, 139MB/s, 32.0MB/s  ]\n",
            "New Data Upload               :  33% 65.9M/201M [00:05<00:04, 31.9MB/s, 12.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  24% 216M/907M [00:05<00:04, 170MB/s, 40.0MB/s  ]\n",
            "New Data Upload               :  36% 73.3M/201M [00:05<00:03, 33.4MB/s, 13.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  28% 257M/907M [00:05<00:03, 180MB/s, 45.9MB/s  ]\n",
            "New Data Upload               :  40% 80.7M/201M [00:05<00:03, 34.4MB/s, 14.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  34% 306M/907M [00:06<00:03, 200MB/s, 52.8MB/s  ]\n",
            "New Data Upload               :  44% 88.1M/201M [00:06<00:03, 35.2MB/s, 15.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  39% 356M/907M [00:06<00:02, 214MB/s, 59.3MB/s  ]\n",
            "New Data Upload               :  47% 95.5M/201M [00:06<00:02, 35.7MB/s, 15.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  44% 402M/907M [00:06<00:02, 219MB/s, 64.8MB/s  ]\n",
            "New Data Upload               :  54% 108M/201M [00:06<00:02, 43.5MB/s, 17.4MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  50% 453M/907M [00:06<00:01, 230MB/s, 70.7MB/s  ]\n",
            "New Data Upload               :  58% 117M/201M [00:06<00:01, 44.3MB/s, 18.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  56% 503M/907M [00:06<00:01, 237MB/s, 76.3MB/s  ]\n",
            "New Data Upload               :  63% 126M/201M [00:06<00:01, 44.0MB/s, 19.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  60% 544M/907M [00:07<00:01, 227MB/s, 80.0MB/s  ]\n",
            "New Data Upload               :  66% 133M/201M [00:07<00:01, 42.1MB/s, 19.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  66% 597M/907M [00:07<00:01, 238MB/s, 85.3MB/s  ]\n",
            "New Data Upload               :  71% 144M/201M [00:07<00:01, 45.3MB/s, 20.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  70% 639M/907M [00:07<00:01, 230MB/s, 88.8MB/s  ]\n",
            "New Data Upload               :  76% 153M/201M [00:07<00:01, 44.7MB/s, 21.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  76% 687M/907M [00:07<00:00, 232MB/s, 92.8MB/s  ]\n",
            "New Data Upload               :  79% 158M/201M [00:07<00:01, 39.7MB/s, 21.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  81% 736M/907M [00:07<00:00, 237MB/s, 96.9MB/s  ]\n",
            "New Data Upload               :  82% 166M/201M [00:07<00:00, 39.0MB/s, 21.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  87% 786M/907M [00:08<00:00, 240MB/s,  101MB/s  ]\n",
            "New Data Upload               :  86% 173M/201M [00:08<00:00, 38.6MB/s, 22.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  91% 826M/907M [00:08<00:00, 229MB/s,  103MB/s  ]\n",
            "New Data Upload               :  89% 180M/201M [00:08<00:00, 37.4MB/s, 22.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  94% 856M/907M [00:08<00:00, 205MB/s,  104MB/s  ]\n",
            "New Data Upload               :  78% 184M/235M [00:08<00:01, 31.8MB/s, 22.4MB/s  ]\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  13% 1.09k/8.61k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  13% 1.09k/8.61k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  95% 861M/907M [00:06<00:00, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  13% 1.09k/8.61k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  95% 861M/907M [00:08<00:00, 151MB/s,  103MB/s  ]\n",
            "New Data Upload               :  81% 189M/235M [00:08<00:01, 30.5MB/s, 22.5MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  95% 865M/907M [00:06<00:00, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  18% 1.56k/8.61k [00:00<00:03, 2.34kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  95% 865M/907M [00:08<00:00, 112MB/s,  101MB/s  ]\n",
            "New Data Upload               :  82% 193M/235M [00:08<00:01, 26.9MB/s, 22.4MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  96% 873M/907M [00:07<00:00, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  29% 2.49k/8.61k [00:00<00:01, 3.51kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  96% 873M/907M [00:09<00:00, 89.7MB/s, 99.2MB/s  ]\n",
            "New Data Upload               :  85% 201M/235M [00:09<00:01, 30.4MB/s, 22.8MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  97% 880M/907M [00:07<00:00, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  40% 3.43k/8.61k [00:00<00:01, 3.89kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  97% 880M/907M [00:09<00:00, 73.8MB/s, 97.8MB/s  ]\n",
            "New Data Upload               :  89% 208M/235M [00:09<00:00, 32.3MB/s, 23.1MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  98% 886M/907M [00:07<00:00, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  45% 3.90k/8.61k [00:00<00:01, 3.51kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  98% 886M/907M [00:09<00:00, 60.1MB/s, 96.3MB/s  ]\n",
            "New Data Upload               :  91% 214M/235M [00:09<00:00, 31.0MB/s, 23.2MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  98% 890M/907M [00:07<00:00, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  51% 4.36k/8.61k [00:01<00:01, 3.27kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  98% 890M/907M [00:09<00:00, 48.5MB/s, 94.7MB/s  ]\n",
            "New Data Upload               :  93% 218M/235M [00:09<00:00, 28.2MB/s, 23.2MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  99% 894M/907M [00:07<00:00, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  62% 5.30k/8.61k [00:01<00:00, 3.50kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  99% 894M/907M [00:09<00:00, 39.4MB/s, 93.1MB/s  ]\n",
            "New Data Upload               :  94% 222M/235M [00:09<00:00, 25.2MB/s, 23.1MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  99% 897M/907M [00:07<00:00, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  72% 6.23k/8.61k [00:01<00:00, 3.67kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  99% 897M/907M [00:10<00:00, 33.1MB/s, 91.6MB/s  ]\n",
            "New Data Upload               :  96% 225M/235M [00:10<00:00, 23.1MB/s, 23.0MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors:  99% 901M/907M [00:08<00:00, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  83% 7.17k/8.61k [00:01<00:00, 3.80kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      :  99% 901M/907M [00:10<00:00, 28.6MB/s, 90.1MB/s  ]\n",
            "New Data Upload               :  97% 229M/235M [00:10<00:00, 21.7MB/s, 22.9MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 904M/907M [00:08<00:00, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors:  94% 8.10k/8.61k [00:01<00:00, 3.90kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      : 100% 904M/907M [00:10<00:00, 25.5MB/s, 88.7MB/s  ]\n",
            "New Data Upload               :  99% 233M/235M [00:10<00:00, 20.7MB/s, 22.8MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 906M/907M [00:08<00:00, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.57k/8.61k [00:01<00:00, 3.74kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      : 100% 906M/907M [00:10<00:00, 20.6MB/s, 88.9MB/s  ]\n",
            "New Data Upload               : 100% 234M/235M [00:10<00:00, 17.2MB/s, 23.0MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 906M/907M [00:08<00:00, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.57k/8.61k [00:02<00:00, 3.40kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.57k/8.61k [00:02<00:00, 3.40kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 101MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.57k/8.61k [00:02<00:00, 3.12kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (0 / 3)      : 100% 907M/907M [00:11<00:00, 11.4MB/s, 88.9MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:11<00:00, 9.55MB/s, 23.0MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 98.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:02<00:00, 2.89kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100% 907M/907M [00:11<00:00, 8.75MB/s, 88.9MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:11<00:00, 7.36MB/s, 23.0MB/s  ]\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 96.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:02<00:00, 2.68kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:02<00:00, 2.68kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 94.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.51kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:02<00:00, 2.51kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 92.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.38kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.38kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 92.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.35kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)      : 100% 907M/907M [00:11<00:00, 76.8MB/s, 90.7MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:11<00:00, 19.9MB/s, 23.5MB/s  ]\n",
            "  ...d_model/model.safetensors: 100% 907M/907M [00:09<00:00, 92.4MB/s]\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.35kB/s]\n",
            "  ...zer_processor.safetensors: 100% 8.61k/8.61k [00:03<00:00, 2.35kB/s]\n",
            "https://huggingface.co/Alkatt/smolvla_so101_CubeToBowl_ASN_Finetuned_V5/tree/main/.\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli upload Alkatt/smolvla_so101_CubeToBowl_ASN_Finetuned_V5 \\\n",
        "  /content/lerobot/outputs/train/smolvla_so101_CubeToBowl_ASN_Finetuned_V5/checkpoints/last/pretrained_model --repo-type model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}